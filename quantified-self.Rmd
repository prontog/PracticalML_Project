---
title: "Quantified-Self Predictions"
author: "Panos Rontogiannis"
date: "`r Sys.Date()`"
output: 
    html_document:
        keep_md: true
---

## Introduction

> Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The data for this project comes from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The goal of the project is to predict the manner in which the participants did the exercise. This is the **classe** variable in the training set. We will try to use some of the remaining variables to predict *classe* using the `caret` R package. 

```{r global_options, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/', warning=FALSE, message=FALSE)

library(caret)
```

## The Data

The data is loaded from two different zipped files 'pml-training.csv.gz' and 'pml-testing.csv.gz' containing the training and testing observations respectively.

```{r load_data, cache = TRUE}
training_orig <- read.csv('pml-training.csv.gz')
testing_final <- read.csv('pml-testing.csv.gz')
```

Initially we have `r nrow(training_orig)` observations for training and `r nrow(testing_final)` for final testing by the project evaluators. Note that this dataset does not contain the `classe` variable. For this reason we will only use this dataset at the end to create the files for the project submission. 

The training dataset has `r ncol(training_orig)` variables (columns) including the response variable (`classe`). Let's split it into two more sets. Firstly the actual training dataset and secondly the testing set to be used to select the best model. This way we will be able to measure the out of sample error.

```{r split_data, cache = TRUE}
set.seed(3456)
inTrain <- createDataPartition(training_orig$classe, p=0.7, list=FALSE)
training <- training_orig[inTrain,]
testing <- training_orig[-inTrain,]
```

So now we have `r nrow(training)` observations for training our models and `r nrow(testing)` for evaluating the trained models and picking the best one.

## Exploratory Analysis

Let's explore the training dataset. First let's have a look at the response variable. It is a factor variable with 5 levels (`r levels(training$classe)`). As can be seen from the frequency table of the types of variable, most of the are numerical:
```{r variable_types_pre}
table(sapply(colnames(training), FUN = function(n) class(training[, n])))
```

Let's continue by looking for missing values.

```{r explor_missing_values_pt1}
complete_training <- complete.cases(training)
```

From the training data we can see that `r sum(complete_training)` out of `r nrow(training)` observations are complete (no missing values at any column). Looking more closely we can see that there are columns that mostly consist of missing values.

```{r explor_missing_values_pt2}
# Count missing values per column.
missing_values <- data.frame(col_name = names(training), na_count = colSums(is.na(training)), row.names = NULL)
missing_values <- subset(missing_values, na_count > 0)

qplot(missing_values$na_count, xlab = '# of NA', ylab = '# of variables', main = 'Exploring missing values', binwidth = 1)

# Remove columns with missing values.
training_no_na <- training[, !(names(training) %in% missing_values$col_name)]
```

After removing those `r nrow(missing_values)` columns we are left with `r ncol(training_no_na)`.

Next let's see if there are any variables with near zero variance.

```{r explor_near_zero_var}
nzv <- nearZeroVar(training_no_na)
# Remove columns with near zero variance.
training_no_na_nzv <- training_no_na[, -nzv]
```

After removing those `r length(nzv)` columns we are left with `r ncol(training_no_na_nzv)`.

Now let's look for high correlations between the remainder variables.

```{r explor_correlations}
# Need to remove factor variables first for the cor function to work.
no_factors <- c()
for (f in colnames(training_no_na_nzv)) {
    if (!is.factor(training_no_na_nzv[, f]))
        no_factors <- c(no_factors, f)
}
cor_training <- cor(training_no_na_nzv[, no_factors])
# find highly correlated descriptors
corrDescr <- findCorrelation(cor_training, cutoff = 0.75)
# remove them from training set
training_no_na_nzv_cor <- training_no_na_nzv[, -corrDescr]
```

After removing those `r length(corrDescr)` columns we are left with `r ncol(training_no_na_nzv_cor)`. These are: *`r sort(names(training_no_na_nzv_cor))`*.

We can see that there are two time-stamp variables, a factor variable called *user_name*, a variable *X* which is simply the row number and finally *num_window*. Let's remove them since they are irrelevant.

```{r explor_timestamps}
training_no_na_nzv_cor <- training_no_na_nzv_cor[, !(names(training_no_na_nzv_cor) %in% c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'user_name', 'X', 'num_window'))]
```

The final dataset before we start training has `r ncol(training_no_na_nzv_cor)` columns (including the response variables). These are *`r sort(names(training_no_na_nzv_cor))`*.

After all this pre-processing, the frequency table of variable types has become:
```{r variable_types_post}
table(sapply(colnames(training_no_na_nzv_cor), FUN = function(n) class(training_no_na_nzv_cor[, n])))
```

## Training

```{r echo=FALSE}
# Prepare for training in parallel (works in Linux).
library(doMC)
registerDoMC(2)
# Cleanup large variables that are not needed anymore.
rm(training_orig, training_no_na, training_no_na_nzv)
```

Because `classe` is a *factor* variable we will need to train a classifier and since it has more than two levels, we cannot use *Generalized Linear Models* (glm). Instead we have to use more advanced classifiers. More specifically *classification and regression trees (CART)*, *stochastic gradient boosting* and *random forest*. All classifiers will be trained using *5-fold Cross Validation* to reduce model overfitting and to estimate the out of sample error during training. We expect this estimate to be close to the final out of sample error that we will calculate later on using the test dataset. Our metric will be *accuracy*.

```{r train_control}
trControl <- trainControl(method="cv", number=5)
```

Let's train the classifiers, print a summary for each model and plot the most important features:

### 1. CART (rpart)

```{r training_rpart, cache=TRUE}
m_no_na_nzv_cor_rpart <- train(classe ~ ., data = training_no_na_nzv_cor, method='rpart', trControl = trControl)

m_no_na_nzv_cor_rpart$results[1,]
```

### 2. Boosting (gbm)

For boosting, the maximum depth of interactions is set to 40, the number of trees to 150 with learning rate of 0.01.

```{r training_gbm, cache=TRUE}
gbmParam <-  expand.grid(interaction.depth = 40, n.trees = 150, shrinkage = .01, n.minobsinnode = 10)
m_no_na_nzv_cor_gbm <- train(classe ~ ., data = training_no_na_nzv_cor, trControl = trControl, tuneGrid = gbmParam, method='gbm', distribution = 'multinomial', verbose=FALSE)

m_no_na_nzv_cor_gbm$results
```

The estimated accuracy of this model is quite high. With more fine tuning it could probably get even higher.

### 3. Random Forest (parRF). 

For the training of the random forest classifier, `mtry` is set to 2 and `proximity` to TRUE.

```{r training_rf, cache=TRUE}
rfParam <- expand.grid(mtry = 2)
m_no_na_nzv_cor_rf <- train(classe ~ ., data = training_no_na_nzv_cor, method='parRF', prox=TRUE, tuneGrid = rfParam, trControl = trControl)

m_no_na_nzv_cor_rf$results
```

## Results

First let's compare all models using the resamples function:

```{r training_comparison, cache=TRUE}
resamps <- resamples(list(RPART = m_no_na_nzv_cor_rpart, RF = m_no_na_nzv_cor_rf, GBM = m_no_na_nzv_cor_gbm))
summary(resamps)
```

We can see that the random forest is the most accurate model followed by boosting and CART. We can also use the testing dataset (taken from the original training set) to see how the three models perform compared to the actual values of the response variable *classe*:

### 1. CART (rpart)
```{r results_rpart}
pred_rpart <- predict(m_no_na_nzv_cor_rpart, testing)
cm_rpart <- confusionMatrix(pred_rpart, testing$classe)
cm_rpart
```

We can see that the out of sample error measured by accuracy is `r cm_rpart$overall[1]`. During training, this was estimated to be `r m_no_na_nzv_cor_rpart$results$Accuracy[1]`. This classifier does not perform well.

### 2. Boosting (gbm)
```{r results_gbm}
pred_gbm <- predict(m_no_na_nzv_cor_gbm, testing)
cm_gbm <- confusionMatrix(pred_gbm, testing$classe)
cm_gbm
```

The accuracy of this model against the testing data is `r cm_gbm$overall[1]`. During training, this was estimated to be `r m_no_na_nzv_cor_gbm$results$Accuracy[1]`.

### 3. Random Forest (rf)
```{r results_rf}
pred_rf <- predict(m_no_na_nzv_cor_rf, testing)
cm_rf <- confusionMatrix(pred_rf, testing$classe)
cm_rf
```

In this case accuracy is `r cm_rf$overall[1]`. During training, this was estimated to be `r m_no_na_nzv_cor_rf$results$Accuracy[1]`. From the table we see that almost all observations were correctly predicted. There just a few misclassifications. Overall this classifier performed exceptionally well outperforming the other two classifiers.

## Summary

To summarize we used the *caret* R package to split the data into two datasets for training and testing. We then preprocessed the training dataset using several techniques (missing values, near zero values, linear correlations) and managed to reduce the number of variables from `r ncol(training)` to `r ncol(training_no_na_nzv_cor)`. Then we trained three different classifiers using the *CART*, *Random Forest* and *Boosting* methods. 5-fold Cross Validation was used to validate each model and avoid overfitting. From these three classifiers, the one based on *random forests* performed significantly better with accuracy close to 99% on the test dataset.

## References

The data for this project come from this [source](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

## Appendix A: Training times

To train in parallel the `doMC` package was used with 2 workers. The total time (in seconds) needed to train each method is:  
1. CART: `r m_no_na_nzv_cor_rpart$times$everything[3]`  
2. Boosting: `r m_no_na_nzv_cor_gbm$times$everything[3]`  
3. Random Forest: `r m_no_na_nzv_cor_rf$times$everything[3]`  

## Appendix B: Selected features

### 1. CART

```{r selected_features_rpart}
plot(varImp(m_no_na_nzv_cor_rpart), top = 10)
```

### 2. Boosting

```{r selected_features_gbm}
plot(varImp(m_no_na_nzv_cor_gbm), top = 10)
```

### 3. Random Forest

```{r selected_features_rf}
plot(varImp(m_no_na_nzv_cor_rf), top = 10)
```

## Appendix C: Submission dataset

For the project submission, the following code was used to create the separate files for each of the 20 test observations:
```{r submission}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

final_pred_rpart <- predict(m_no_na_nzv_cor_rpart, testing_final)
final_pred_gbm <- predict(m_no_na_nzv_cor_gbm, testing_final)
final_pred_rf <- predict(m_no_na_nzv_cor_rf, testing_final)

data.frame(RF = final_pred_rf, GBM = final_pred_gbm, CART = final_pred_rpart)
```

## Appendix D: Session Info

Session Info:  
```{r echo=FALSE} 
sessionInfo()
```